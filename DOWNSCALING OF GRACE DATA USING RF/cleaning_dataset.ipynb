{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecb052d-162b-4441-8147-a455f58dfd72",
   "metadata": {},
   "source": [
    "# GLDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f4911c3d-506f-45f0-bdb3-f99f7a3a6d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful! DataFrame shape: (156492, 4)\n",
      "   CanopInt_inst_mean  Longitude   Latitude     Date\n",
      "0                 NaN  67.822804  31.890193  2003-01\n",
      "1                 NaN  68.271962  31.890193  2003-01\n",
      "2                 NaN  68.721119  31.890193  2003-01\n",
      "3                 NaN  69.170277  31.890193  2003-01\n",
      "4                 NaN  69.619435  31.890193  2003-01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import re  # To extract year and month from filenames\n",
    "\n",
    "# Define TIFF folder path\n",
    "tiff_folder = \"TIFF_DATASET\\\\gldas_tiff\"\n",
    "\n",
    "# Get all TIFF files\n",
    "tiff_files = sorted(glob.glob(os.path.join(tiff_folder, \"*.tif\")))\n",
    "\n",
    "# Debug: Ensure files are found\n",
    "if not tiff_files:\n",
    "    raise FileNotFoundError(\"No TIFF files found. Check folder path and file format.\")\n",
    "\n",
    "# Initialize data list\n",
    "data_list = []\n",
    "\n",
    "for tiff_file in tiff_files:\n",
    "    # Extract year and month from filename using regex\n",
    "    match = re.search(r\"_(\\d{4})_(\\d{1,2})\", os.path.basename(tiff_file))\n",
    "    if match:\n",
    "        year, month = match.groups()\n",
    "        date_str = f\"{year}-{int(month):02d}\"  # Format as YYYY-MM\n",
    "    else:\n",
    "        print(f\"Skipping file (invalid name format): {tiff_file}\")\n",
    "        continue\n",
    "\n",
    "    # Open TIFF file\n",
    "    dataset = gdal.Open(tiff_file)\n",
    "    if dataset is None:\n",
    "        print(f\"Error: Could not open {tiff_file}\")\n",
    "        continue\n",
    "\n",
    "    # Get band names from the dataset metadata (if available)\n",
    "    band_names = [dataset.GetRasterBand(i + 1).GetDescription() for i in range(dataset.RasterCount)]\n",
    "    \n",
    "    # Read all bands into a list of arrays\n",
    "    band_arrays = [dataset.GetRasterBand(i + 1).ReadAsArray() for i in range(dataset.RasterCount)]\n",
    "    \n",
    "    if any(array is None for array in band_arrays):\n",
    "        print(f\"Error: No data in bands of {tiff_file}\")\n",
    "        continue\n",
    "    \n",
    "    transform = dataset.GetGeoTransform()\n",
    "    \n",
    "    rows, cols = band_arrays[0].shape\n",
    "    latitudes = np.array([transform[3] + row * transform[5] for row in range(rows)])\n",
    "    longitudes = np.array([transform[0] + col * transform[1] for col in range(cols)])\n",
    "    \n",
    "    lon_mesh, lat_mesh = np.meshgrid(longitudes, latitudes)\n",
    "    \n",
    "    # Create a DataFrame for each band\n",
    "    band_dict = {}\n",
    "    for i, band_array in enumerate(band_arrays):\n",
    "        band_dict[band_names[i] if band_names[i] else f\"Band_{i+1}\"] = band_array.ravel()\n",
    "\n",
    "    band_dict[\"Longitude\"] = lon_mesh.ravel()\n",
    "    band_dict[\"Latitude\"] = lat_mesh.ravel()\n",
    "    band_dict[\"Date\"] = date_str\n",
    "\n",
    "    df = pd.DataFrame(band_dict)\n",
    "    \n",
    "    data_list.append(df)\n",
    "\n",
    "# Check if data_list has data before concatenating\n",
    "if not data_list:\n",
    "    raise ValueError(\"No valid data was extracted from TIFF files.\")\n",
    "\n",
    "final_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Print DataFrame shape and first few rows to confirm\n",
    "print(\"Conversion successful! DataFrame shape:\", final_df.shape)\n",
    "print(final_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ec834a7a-2970-4bfa-81c5-83abc8ea5d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JT00007\\AppData\\Local\\Temp\\ipykernel_12160\\853554686.py:18: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  final_gdf = final_gdf[final_gdf.within(shapefile.unary_union)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CanopInt_inst_mean  Longitude   Latitude     Date\n",
      "40                0.025629  73.661853  31.441035  2003-01\n",
      "41                0.052762  74.111011  31.441035  2003-01\n",
      "42                0.060323  74.560169  31.441035  2003-01\n",
      "65                0.006956  72.763538  30.991877  2003-01\n",
      "66                0.009871  73.212696  30.991877  2003-01\n",
      "...                    ...        ...        ...      ...\n",
      "156478            0.155813  73.661853  22.008724  2023-09\n",
      "156479            0.164317  74.111011  22.008724  2023-09\n",
      "156480            0.168146  74.560169  22.008724  2023-09\n",
      "156481            0.155529  75.009326  22.008724  2023-09\n",
      "156482                 NaN  75.458484  22.008724  2023-09\n",
      "\n",
      "[95508 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the shapefile (update the file path)\n",
    "shapefile_path = \"RJ_150KM_Buff_Bnd/IND_Rajasthan_Buffer_150km.shp\"\n",
    "shapefile = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Convert final_df to a GeoDataFrame\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(final_df['Longitude'], final_df['Latitude'])]\n",
    "final_gdf = gpd.GeoDataFrame(final_df, geometry=geometry, crs=\"EPSG:4326\")  # Assuming WGS84 projection\n",
    "\n",
    "# Ensure the shapefile is in the same CRS\n",
    "if shapefile.crs != final_gdf.crs:\n",
    "    shapefile = shapefile.to_crs(final_gdf.crs)\n",
    "\n",
    "# Perform spatial join to filter points inside the shapefile boundary\n",
    "final_gdf = final_gdf[final_gdf.within(shapefile.unary_union)]\n",
    "\n",
    "# Convert back to DataFrame (if you don't need the geometry column)\n",
    "final_df = final_gdf.drop(columns=['geometry'])\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7b7734c8-311b-4b6d-beb8-da8f96c02796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'Value' column: 540\n"
     ]
    }
   ],
   "source": [
    "nan_count = final_df[\"CanopInt_inst_mean\"].isna().sum()\n",
    "print(f\"Number of NaN values in 'Value' column: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a02a467f-b2b3-4f01-ae57-db880a0a2fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CanopInt_inst_mean  Longitude   Latitude     Date\n",
      "0                0.025629  73.661853  31.441035  2003-01\n",
      "1                0.052762  74.111011  31.441035  2003-01\n",
      "2                0.060323  74.560169  31.441035  2003-01\n",
      "3                0.006956  72.763538  30.991877  2003-01\n",
      "4                0.009871  73.212696  30.991877  2003-01\n",
      "...                   ...        ...        ...      ...\n",
      "95251            0.150879  75.907642  22.457882  2023-09\n",
      "95252            0.155813  73.661853  22.008724  2023-09\n",
      "95253            0.164317  74.111011  22.008724  2023-09\n",
      "95254            0.168146  74.560169  22.008724  2023-09\n",
      "95255            0.155529  75.009326  22.008724  2023-09\n",
      "\n",
      "[95256 rows x 4 columns]\n",
      "Number of NaN values in 'Value' column: 288\n"
     ]
    }
   ],
   "source": [
    "# Define a small tolerance to account for floating point precision issues\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Remove rows where Longitude and Latitude match within the tolerance range\n",
    "final_df = final_df[\n",
    "    ~(\n",
    "        (final_df[\"Longitude\"].sub(75.458484).abs() < tolerance) & \n",
    "        (final_df[\"Latitude\"].sub(22.008724).abs() < tolerance)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Reset index after removal\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Display updated dataframe\n",
    "print(final_df)\n",
    "\n",
    "\n",
    "nan_count = final_df[\"CanopInt_inst_mean\"].isna().sum()\n",
    "print(f\"Number of NaN values in 'Value' column: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "35eb0244-1adf-4328-a769-b25a29f55dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CanopInt_inst_mean  Longitude   Latitude     Date\n",
      "77394                 NaN  70.966907  24.703670  2020-01\n",
      "77412                 NaN  70.068592  24.254513  2020-01\n",
      "77413                 NaN  70.517750  24.254513  2020-01\n",
      "77414                 NaN  70.966907  24.254513  2020-01\n",
      "77431                 NaN  70.068592  23.805355  2020-01\n",
      "...                   ...        ...        ...      ...\n",
      "95178                 NaN  70.068592  24.254513  2023-09\n",
      "95179                 NaN  70.517750  24.254513  2023-09\n",
      "95180                 NaN  70.966907  24.254513  2023-09\n",
      "95197                 NaN  70.068592  23.805355  2023-09\n",
      "95199                 NaN  70.966907  23.805355  2023-09\n",
      "\n",
      "[288 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'Value' column is NaN\n",
    "nan_rows = final_df[final_df['CanopInt_inst_mean'].isna()]\n",
    "\n",
    "# Display the rows with NaN values in the 'Value' column\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9efbbb17-e2ce-4a73-ad70-35f5f2dde14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spatial Interpolation Completed! Missing Values Remaining: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def inverse_distance_weighting(df, missing_idx, known_coords, known_values, power=2):\n",
    "    \"\"\"\n",
    "    Performs Inverse Distance Weighting (IDW) for spatial interpolation.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset containing Longitude, Latitude, and Value columns.\n",
    "    missing_idx (Index): Indices of missing values.\n",
    "    known_coords (ndarray): Array of known Longitude and Latitude coordinates.\n",
    "    known_values (ndarray): Array of known values.\n",
    "    power (int): The power parameter for IDW (higher = stronger weighting on closer points).\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The dataset with interpolated values.\n",
    "    \"\"\"\n",
    "    # Build spatial tree for fast nearest neighbor search\n",
    "    tree = cKDTree(known_coords)\n",
    "    \n",
    "    for idx in missing_idx:\n",
    "        missing_coord = np.array([df.loc[idx, \"Longitude\"], df.loc[idx, \"Latitude\"]])\n",
    "        \n",
    "        # Find the 5 nearest neighbors\n",
    "        distances, indices = tree.query(missing_coord, k=5)\n",
    "        \n",
    "        # Handle edge case where all distances are 0 (exact match)\n",
    "        if np.any(distances == 0):\n",
    "            df.loc[idx, \"CanopInt_inst_mean\"] = known_values[indices[0]]\n",
    "            continue\n",
    "        \n",
    "        # Compute IDW weights (inverse of distance squared)\n",
    "        weights = 1 / (distances ** power)\n",
    "        weights /= weights.sum()  # Normalize\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        df.loc[idx, \"CanopInt_inst_mean\"] = np.sum(weights * known_values[indices])\n",
    "\n",
    "    return df\n",
    "\n",
    "def spatial_interpolation(df):\n",
    "    \"\"\"\n",
    "    Performs spatial interpolation for missing values using IDW.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset containing Longitude, Latitude, and Value columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The dataset with interpolated values.\n",
    "    \"\"\"\n",
    "    # Ensure Longitude and Latitude are float for spatial calculations\n",
    "    df[\"Longitude\"] = df[\"Longitude\"].astype(float)\n",
    "    df[\"Latitude\"] = df[\"Latitude\"].astype(float)\n",
    "\n",
    "    # Identify missing values\n",
    "    missing_idx = df[df[\"CanopInt_inst_mean\"].isna()].index\n",
    "\n",
    "    # Extract known (non-NaN) values and coordinates\n",
    "    known_df = df.dropna(subset=[\"CanopInt_inst_mean\"])\n",
    "    known_coords = known_df[[\"Longitude\", \"Latitude\"]].values\n",
    "    known_values = known_df[\"CanopInt_inst_mean\"].values\n",
    "\n",
    "    # Apply IDW interpolation\n",
    "    df = inverse_distance_weighting(df, missing_idx, known_coords, known_values)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply spatial interpolation\n",
    "final_df = spatial_interpolation(final_df)\n",
    "\n",
    "# Check missing values after interpolation\n",
    "print(\"✅ Spatial Interpolation Completed! Missing Values Remaining:\", final_df[\"CanopInt_inst_mean\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "20fa8226-f96a-4caf-b8ff-61dd0d10d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [CanopInt_inst_mean, Longitude, Latitude, Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'Value' column is NaN\n",
    "nan_count = final_df[\"CanopInt_inst_mean\"].isna().sum()\n",
    "nan_rows = final_df[final_df['CanopInt_inst_mean'].isna()]\n",
    "\n",
    "# Display the rows with NaN values in the 'Value' column\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "714fca61-7f41-414c-97f2-dbe1dbc84bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CanopInt_inst_mean</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025629</td>\n",
       "      <td>73.661853</td>\n",
       "      <td>31.441035</td>\n",
       "      <td>2003-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052762</td>\n",
       "      <td>74.111011</td>\n",
       "      <td>31.441035</td>\n",
       "      <td>2003-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.060323</td>\n",
       "      <td>74.560169</td>\n",
       "      <td>31.441035</td>\n",
       "      <td>2003-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006956</td>\n",
       "      <td>72.763538</td>\n",
       "      <td>30.991877</td>\n",
       "      <td>2003-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009871</td>\n",
       "      <td>73.212696</td>\n",
       "      <td>30.991877</td>\n",
       "      <td>2003-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95251</th>\n",
       "      <td>0.150879</td>\n",
       "      <td>75.907642</td>\n",
       "      <td>22.457882</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95252</th>\n",
       "      <td>0.155813</td>\n",
       "      <td>73.661853</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95253</th>\n",
       "      <td>0.164317</td>\n",
       "      <td>74.111011</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95254</th>\n",
       "      <td>0.168146</td>\n",
       "      <td>74.560169</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95255</th>\n",
       "      <td>0.155529</td>\n",
       "      <td>75.009326</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95256 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CanopInt_inst_mean  Longitude   Latitude     Date\n",
       "0                0.025629  73.661853  31.441035  2003-01\n",
       "1                0.052762  74.111011  31.441035  2003-01\n",
       "2                0.060323  74.560169  31.441035  2003-01\n",
       "3                0.006956  72.763538  30.991877  2003-01\n",
       "4                0.009871  73.212696  30.991877  2003-01\n",
       "...                   ...        ...        ...      ...\n",
       "95251            0.150879  75.907642  22.457882  2023-09\n",
       "95252            0.155813  73.661853  22.008724  2023-09\n",
       "95253            0.164317  74.111011  22.008724  2023-09\n",
       "95254            0.168146  74.560169  22.008724  2023-09\n",
       "95255            0.155529  75.009326  22.008724  2023-09\n",
       "\n",
       "[95256 rows x 4 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "20acb1a6-ed59-480c-8383-ed0a924f237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('cleaned_gldas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef637183-9d7b-40b6-8684-64684a53c336",
   "metadata": {},
   "source": [
    "# CHIRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "da0c60a1-961b-4c6c-8f61-efb98e796de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful! DataFrame shape: (156492, 4)\n",
      "   precipitation  Longitude   Latitude     Date\n",
      "0            NaN  67.822804  31.890193  2003-01\n",
      "1            NaN  68.271962  31.890193  2003-01\n",
      "2            NaN  68.721119  31.890193  2003-01\n",
      "3            NaN  69.170277  31.890193  2003-01\n",
      "4            NaN  69.619435  31.890193  2003-01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import re  # To extract year and month from filenames\n",
    "\n",
    "# Define TIFF folder path\n",
    "tiff_folder = \"TIFF_DATASET\\\\chirps_tiff\"\n",
    "\n",
    "# Get all TIFF files\n",
    "tiff_files = sorted(glob.glob(os.path.join(tiff_folder, \"*.tif\")))\n",
    "\n",
    "# Debug: Ensure files are found\n",
    "if not tiff_files:\n",
    "    raise FileNotFoundError(\"No TIFF files found. Check folder path and file format.\")\n",
    "\n",
    "# Initialize data list\n",
    "data_list = []\n",
    "\n",
    "for tiff_file in tiff_files:\n",
    "    # Extract year and month from filename using regex\n",
    "    match = re.search(r\"_(\\d{4})_(\\d{1,2})\", os.path.basename(tiff_file))\n",
    "    if match:\n",
    "        year, month = match.groups()\n",
    "        date_str = f\"{year}-{int(month):02d}\"  # Format as YYYY-MM\n",
    "    else:\n",
    "        print(f\"Skipping file (invalid name format): {tiff_file}\")\n",
    "        continue\n",
    "\n",
    "    # Open TIFF file\n",
    "    dataset = gdal.Open(tiff_file)\n",
    "    if dataset is None:\n",
    "        print(f\"Error: Could not open {tiff_file}\")\n",
    "        continue\n",
    "\n",
    "    # Get band names from the dataset metadata (if available)\n",
    "    band_names = [dataset.GetRasterBand(i + 1).GetDescription() for i in range(dataset.RasterCount)]\n",
    "    \n",
    "    # Read all bands into a list of arrays\n",
    "    band_arrays = [dataset.GetRasterBand(i + 1).ReadAsArray() for i in range(dataset.RasterCount)]\n",
    "    \n",
    "    if any(array is None for array in band_arrays):\n",
    "        print(f\"Error: No data in bands of {tiff_file}\")\n",
    "        continue\n",
    "    \n",
    "    transform = dataset.GetGeoTransform()\n",
    "    \n",
    "    rows, cols = band_arrays[0].shape\n",
    "    latitudes = np.array([transform[3] + row * transform[5] for row in range(rows)])\n",
    "    longitudes = np.array([transform[0] + col * transform[1] for col in range(cols)])\n",
    "    \n",
    "    lon_mesh, lat_mesh = np.meshgrid(longitudes, latitudes)\n",
    "    \n",
    "    # Create a DataFrame for each band\n",
    "    band_dict = {}\n",
    "    for i, band_array in enumerate(band_arrays):\n",
    "        band_dict[band_names[i] if band_names[i] else f\"Band_{i+1}\"] = band_array.ravel()\n",
    "\n",
    "    band_dict[\"Longitude\"] = lon_mesh.ravel()\n",
    "    band_dict[\"Latitude\"] = lat_mesh.ravel()\n",
    "    band_dict[\"Date\"] = date_str\n",
    "\n",
    "    df = pd.DataFrame(band_dict)\n",
    "    \n",
    "    data_list.append(df)\n",
    "\n",
    "# Check if data_list has data before concatenating\n",
    "if not data_list:\n",
    "    raise ValueError(\"No valid data was extracted from TIFF files.\")\n",
    "\n",
    "final_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Print DataFrame shape and first few rows to confirm\n",
    "print(\"Conversion successful! DataFrame shape:\", final_df.shape)\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f007bcfc-dd8a-46fb-93e7-6beb8aef80f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JT00007\\AppData\\Local\\Temp\\ipykernel_12160\\853554686.py:18: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  final_gdf = final_gdf[final_gdf.within(shapefile.unary_union)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        precipitation  Longitude   Latitude     Date\n",
      "40          11.927373  73.661853  31.441035  2003-01\n",
      "41          11.360927  74.111011  31.441035  2003-01\n",
      "42          13.307369  74.560169  31.441035  2003-01\n",
      "65          11.916802  72.763538  30.991877  2003-01\n",
      "66           9.952769  73.212696  30.991877  2003-01\n",
      "...               ...        ...        ...      ...\n",
      "156478       4.361670  73.661853  22.008724  2023-12\n",
      "156479       3.835233  74.111011  22.008724  2023-12\n",
      "156480       3.413360  74.560169  22.008724  2023-12\n",
      "156481       6.411314  75.009326  22.008724  2023-12\n",
      "156482            NaN  75.458484  22.008724  2023-12\n",
      "\n",
      "[95508 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the shapefile (update the file path)\n",
    "shapefile_path = \"RJ_150KM_Buff_Bnd/IND_Rajasthan_Buffer_150km.shp\"\n",
    "shapefile = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Convert final_df to a GeoDataFrame\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(final_df['Longitude'], final_df['Latitude'])]\n",
    "final_gdf = gpd.GeoDataFrame(final_df, geometry=geometry, crs=\"EPSG:4326\")  # Assuming WGS84 projection\n",
    "\n",
    "# Ensure the shapefile is in the same CRS\n",
    "if shapefile.crs != final_gdf.crs:\n",
    "    shapefile = shapefile.to_crs(final_gdf.crs)\n",
    "\n",
    "# Perform spatial join to filter points inside the shapefile boundary\n",
    "final_gdf = final_gdf[final_gdf.within(shapefile.unary_union)]\n",
    "\n",
    "# Convert back to DataFrame (if you don't need the geometry column)\n",
    "final_df = final_gdf.drop(columns=['geometry'])\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "84dcc15f-8e34-4b83-8267-020e6d686a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'Value' column: 252\n"
     ]
    }
   ],
   "source": [
    "nan_count = final_df[\"precipitation\"].isna().sum()\n",
    "print(f\"Number of NaN values in 'Value' column: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e9b32f7c-eaec-4060-80f6-8c3de8086e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       precipitation  Longitude   Latitude     Date\n",
      "0          11.927373  73.661853  31.441035  2003-01\n",
      "1          11.360927  74.111011  31.441035  2003-01\n",
      "2          13.307369  74.560169  31.441035  2003-01\n",
      "3          11.916802  72.763538  30.991877  2003-01\n",
      "4           9.952769  73.212696  30.991877  2003-01\n",
      "...              ...        ...        ...      ...\n",
      "95251       6.139013  75.907642  22.457882  2023-12\n",
      "95252       4.361670  73.661853  22.008724  2023-12\n",
      "95253       3.835233  74.111011  22.008724  2023-12\n",
      "95254       3.413360  74.560169  22.008724  2023-12\n",
      "95255       6.411314  75.009326  22.008724  2023-12\n",
      "\n",
      "[95256 rows x 4 columns]\n",
      "Number of NaN values in 'Value' column: 0\n"
     ]
    }
   ],
   "source": [
    "# Define a small tolerance to account for floating point precision issues\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Remove rows where Longitude and Latitude match within the tolerance range\n",
    "final_df = final_df[\n",
    "    ~(\n",
    "        (final_df[\"Longitude\"].sub(75.458484).abs() < tolerance) & \n",
    "        (final_df[\"Latitude\"].sub(22.008724).abs() < tolerance)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Reset index after removal\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Display updated dataframe\n",
    "print(final_df)\n",
    "\n",
    "\n",
    "nan_count = final_df[\"precipitation\"].isna().sum()\n",
    "print(f\"Number of NaN values in 'Value' column: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "afdbd0ab-af7a-4148-a43c-1dd6dd7a26ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [precipitation, Longitude, Latitude, Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'Value' column is NaN\n",
    "nan_rows = final_df[final_df['precipitation'].isna()]\n",
    "\n",
    "# Display the rows with NaN values in the 'Value' column\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a6038a52-4f40-45ab-b2a8-ddc596149da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('cleaned_chirps.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ff9d2-74e6-4504-84a7-d17768ecaaca",
   "metadata": {},
   "source": [
    "# GRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d5a381c4-0157-46e3-8cc0-30abcfec7b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful! DataFrame shape: (130410, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import re  # To extract year and month from filenames\n",
    "\n",
    "# Define TIFF folder path\n",
    "tiff_folder = \"TIFF_DATASET\\\\grace_tiff\"\n",
    "\n",
    "# Get all TIFF files\n",
    "tiff_files = sorted(glob.glob(os.path.join(tiff_folder, \"*.tif\")))\n",
    "\n",
    "# Debug: Ensure files are found\n",
    "if not tiff_files:\n",
    "    raise FileNotFoundError(\"No TIFF files found. Check folder path and file format.\")\n",
    "\n",
    "# Initialize data list\n",
    "data_list = []\n",
    "\n",
    "for tiff_file in tiff_files:\n",
    "    # Extract year and month from filename using regex\n",
    "    match = re.search(r\"_(\\d{4})_(\\d{1,2})\", os.path.basename(tiff_file))\n",
    "    if match:\n",
    "        year, month = match.groups()\n",
    "        date_str = f\"{year}-{int(month):02d}\"  # Format as YYYY-MM\n",
    "    else:\n",
    "        print(f\"Skipping file (invalid name format): {tiff_file}\")\n",
    "        continue\n",
    "\n",
    "    # Open TIFF file\n",
    "    dataset = gdal.Open(tiff_file)\n",
    "    if dataset is None:\n",
    "        print(f\"Error: Could not open {tiff_file}\")\n",
    "        continue\n",
    "    \n",
    "    # Get band names from the dataset metadata (if available)\n",
    "    band_names = [dataset.GetRasterBand(i + 1).GetDescription() for i in range(dataset.RasterCount)]\n",
    "    \n",
    "    # Read all bands into a list of arrays\n",
    "    band_arrays = [dataset.GetRasterBand(i + 1).ReadAsArray() for i in range(dataset.RasterCount)]\n",
    "    \n",
    "    if any(array is None for array in band_arrays):\n",
    "        print(f\"Error: No data in bands of {tiff_file}\")\n",
    "        continue\n",
    "    \n",
    "    transform = dataset.GetGeoTransform()\n",
    "    \n",
    "    rows, cols = band_arrays[0].shape\n",
    "    latitudes = np.array([transform[3] + row * transform[5] for row in range(rows)])\n",
    "    longitudes = np.array([transform[0] + col * transform[1] for col in range(cols)])\n",
    "    \n",
    "    lon_mesh, lat_mesh = np.meshgrid(longitudes, latitudes)\n",
    "    \n",
    "    # Create a DataFrame for each band\n",
    "    band_dict = {}\n",
    "    for i, band_array in enumerate(band_arrays):\n",
    "        band_dict[band_names[i] if band_names[i] else f\"Band_{i+1}\"] = band_array.ravel()\n",
    "\n",
    "    band_dict[\"Longitude\"] = lon_mesh.ravel()\n",
    "    band_dict[\"Latitude\"] = lat_mesh.ravel()\n",
    "    band_dict[\"Date\"] = date_str\n",
    "\n",
    "    df = pd.DataFrame(band_dict)\n",
    "    \n",
    "    data_list.append(df)\n",
    "\n",
    "# Check if data_list has data before concatenating\n",
    "if not data_list:\n",
    "    raise ValueError(\"No valid data was extracted from TIFF files.\")\n",
    "\n",
    "final_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame (optional, you can save it to CSV if needed)\n",
    "print(\"Conversion successful! DataFrame shape:\", final_df.shape)\n",
    "#print(final_df.head())\n",
    "\n",
    "# Optionally, save to CSV if needed\n",
    "# final_df.to_csv(\"GRACE_tiff_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9ceedaae-262c-4604-a642-1388e311d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JT00007\\AppData\\Local\\Temp\\ipykernel_12160\\853554686.py:18: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  final_gdf = final_gdf[final_gdf.within(shapefile.unary_union)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lwe_thickness_mean  Longitude   Latitude     Date\n",
      "40               16.868571  73.661853  31.441035  2003-01\n",
      "41               16.868571  74.111011  31.441035  2003-01\n",
      "42               16.868571  74.560169  31.441035  2003-01\n",
      "65                2.626078  72.763538  30.991877  2003-01\n",
      "66                2.626078  73.212696  30.991877  2003-01\n",
      "...                    ...        ...        ...      ...\n",
      "130396           17.498751  73.661853  22.008724  2023-12\n",
      "130397           15.043857  74.111011  22.008724  2023-12\n",
      "130398           15.043857  74.560169  22.008724  2023-12\n",
      "130399           15.043857  75.009326  22.008724  2023-12\n",
      "130400                 NaN  75.458484  22.008724  2023-12\n",
      "\n",
      "[79590 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the shapefile (update the file path)\n",
    "shapefile_path = \"RJ_150KM_Buff_Bnd/IND_Rajasthan_Buffer_150km.shp\"\n",
    "shapefile = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Convert final_df to a GeoDataFrame\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(final_df['Longitude'], final_df['Latitude'])]\n",
    "final_gdf = gpd.GeoDataFrame(final_df, geometry=geometry, crs=\"EPSG:4326\")  # Assuming WGS84 projection\n",
    "\n",
    "# Ensure the shapefile is in the same CRS\n",
    "if shapefile.crs != final_gdf.crs:\n",
    "    shapefile = shapefile.to_crs(final_gdf.crs)\n",
    "\n",
    "# Perform spatial join to filter points inside the shapefile boundary\n",
    "final_gdf = final_gdf[final_gdf.within(shapefile.unary_union)]\n",
    "\n",
    "# Convert back to DataFrame (if you don't need the geometry column)\n",
    "final_df = final_gdf.drop(columns=['geometry'])\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "88b95964-8bed-467e-96e7-c408bb47a0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       lwe_thickness_mean  Longitude   Latitude     Date\n",
      "0               -1.616644  68.271962  26.051143  2003-01\n",
      "1               -1.616644  68.271962  26.500301  2003-01\n",
      "2               -1.616644  68.271962  26.949459  2003-01\n",
      "3               -1.616644  68.271962  27.398616  2003-01\n",
      "4               -1.616644  68.271962  27.847774  2003-01\n",
      "...                   ...        ...        ...      ...\n",
      "95503          -23.875206  79.051745  27.398616  2023-12\n",
      "95504          -23.875206  79.051745  27.847774  2023-12\n",
      "95505          -23.875206  79.500903  26.500301  2023-12\n",
      "95506          -23.875206  79.500903  26.949459  2023-12\n",
      "95507          -23.875206  79.500903  27.398616  2023-12\n",
      "\n",
      "[95508 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Create a list of unique latitudes and longitudes\n",
    "lat_long_pairs = final_df[['Longitude', 'Latitude']].drop_duplicates()\n",
    "\n",
    "# Define the full list of months and years in the desired format (e.g., YYYY-MM)\n",
    "years = list(range(2003, 2023 + 1))  # Adjust as needed\n",
    "months = list(range(1, 13))\n",
    "\n",
    "# Create all possible 'YYYY-MM' combinations\n",
    "full_months = [f'{year}-{str(month).zfill(2)}' for year in years for month in months]\n",
    "\n",
    "# Get the existing months in the dataset\n",
    "existing_months = final_df['Date'].unique().tolist()\n",
    "\n",
    "# Find the missing months\n",
    "missing_months = list(set(full_months) - set(existing_months))\n",
    "\n",
    "# Prepare a DataFrame for the missing months with NaN values\n",
    "missing_rows = []\n",
    "\n",
    "for missing_month in missing_months:\n",
    "    for _, row in lat_long_pairs.iterrows():\n",
    "        missing_rows.append({\n",
    "            'Longitude': row['Longitude'],\n",
    "            'Latitude': row['Latitude'],\n",
    "            'lwe_thickness_mean': np.nan,\n",
    "            'Date': missing_month\n",
    "        })\n",
    "\n",
    "# Convert the missing rows to a DataFrame\n",
    "missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "# Concatenate the missing rows with the original final_df\n",
    "final_df = pd.concat([final_df, missing_df], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame to maintain the original order (by 'Date', 'Longitude', 'Latitude')\n",
    "final_df = final_df.sort_values(by=['Date', 'Longitude', 'Latitude']).reset_index(drop=True)\n",
    "\n",
    "# Display the updated final_df\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "15019c30-2d11-4170-90b9-c968e6014c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       lwe_thickness_mean  Longitude   Latitude       Date\n",
      "0               -1.616644  68.271962  26.051143 2003-01-01\n",
      "1               -0.892126  68.271962  26.051143 2003-02-01\n",
      "2               -2.181170  68.271962  26.051143 2003-03-01\n",
      "3               -0.756670  68.271962  26.051143 2003-04-01\n",
      "4                0.975714  68.271962  26.051143 2003-05-01\n",
      "...                   ...        ...        ...        ...\n",
      "95503           -1.008954  79.500903  27.398616 2023-08-01\n",
      "95504           -4.776188  79.500903  27.398616 2023-09-01\n",
      "95505          -10.928502  79.500903  27.398616 2023-10-01\n",
      "95506          -18.515027  79.500903  27.398616 2023-11-01\n",
      "95507          -23.875206  79.500903  27.398616 2023-12-01\n",
      "\n",
      "[95508 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JT00007\\AppData\\Local\\Temp\\ipykernel_12160\\3706800949.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby([\"Longitude\", \"Latitude\"]).apply(interpolate_group).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def temporal_interpolation(df):\n",
    "    # Sort by Longitude, Latitude, and Date to maintain correct order\n",
    "    df = df.sort_values(by=[\"Longitude\", \"Latitude\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "    def interpolate_group(group):\n",
    "        group = group.copy()  # Avoid modifying original group\n",
    "        group[\"lwe_thickness_mean\"] = group[\"lwe_thickness_mean\"].astype(float)  # Ensure numeric\n",
    "\n",
    "        for idx in range(len(group)):\n",
    "            if pd.isna(group.iloc[idx][\"lwe_thickness_mean\"]):  # Only interpolate if Value is NaN\n",
    "                neighbors = []\n",
    "\n",
    "                # Loop to consider up to 3 previous and next months\n",
    "                for n in range(1, 4):  \n",
    "                    prev_idx = idx - n  # Index of previous n-th month\n",
    "                    next_idx = idx + n  # Index of next n-th month\n",
    "\n",
    "                    # Collect values from previous months\n",
    "                    if prev_idx >= 0:\n",
    "                        prev_value = group.iloc[prev_idx][\"lwe_thickness_mean\"]\n",
    "                        if not pd.isna(prev_value):\n",
    "                            neighbors.append(prev_value)\n",
    "\n",
    "                    # Collect values from next months\n",
    "                    if next_idx < len(group):\n",
    "                        next_value = group.iloc[next_idx][\"lwe_thickness_mean\"]\n",
    "                        if not pd.isna(next_value):\n",
    "                            neighbors.append(next_value)\n",
    "\n",
    "                    # If we have collected enough neighbors (up to 3 previous and 3 next)\n",
    "                    if len(neighbors) > 0:\n",
    "                        # Take the mean of available neighbors\n",
    "                        group.iloc[idx, group.columns.get_loc(\"lwe_thickness_mean\")] = sum(neighbors) / len(neighbors)\n",
    "                        break  # Stop once interpolation is done\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Apply interpolation for each unique coordinate (Longitude, Latitude)\n",
    "    df = df.groupby([\"Longitude\", \"Latitude\"]).apply(interpolate_group).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "final_df[\"Date\"] = pd.to_datetime(final_df[\"Date\"], format=\"%Y-%m\")  # Ensure Date is in datetime format\n",
    "final_df = temporal_interpolation(final_df)\n",
    "\n",
    "# Display the result\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b6885e83-691e-437e-851f-9b9284b67e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lwe_thickness_mean</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.616644</td>\n",
       "      <td>68.271962</td>\n",
       "      <td>26.051143</td>\n",
       "      <td>2003-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.892126</td>\n",
       "      <td>68.271962</td>\n",
       "      <td>26.051143</td>\n",
       "      <td>2003-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.181170</td>\n",
       "      <td>68.271962</td>\n",
       "      <td>26.051143</td>\n",
       "      <td>2003-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.756670</td>\n",
       "      <td>68.271962</td>\n",
       "      <td>26.051143</td>\n",
       "      <td>2003-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.975714</td>\n",
       "      <td>68.271962</td>\n",
       "      <td>26.051143</td>\n",
       "      <td>2003-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95503</th>\n",
       "      <td>-1.008954</td>\n",
       "      <td>79.500903</td>\n",
       "      <td>27.398616</td>\n",
       "      <td>2023-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95504</th>\n",
       "      <td>-4.776188</td>\n",
       "      <td>79.500903</td>\n",
       "      <td>27.398616</td>\n",
       "      <td>2023-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95505</th>\n",
       "      <td>-10.928502</td>\n",
       "      <td>79.500903</td>\n",
       "      <td>27.398616</td>\n",
       "      <td>2023-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95506</th>\n",
       "      <td>-18.515027</td>\n",
       "      <td>79.500903</td>\n",
       "      <td>27.398616</td>\n",
       "      <td>2023-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95507</th>\n",
       "      <td>-23.875206</td>\n",
       "      <td>79.500903</td>\n",
       "      <td>27.398616</td>\n",
       "      <td>2023-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95508 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lwe_thickness_mean  Longitude   Latitude       Date\n",
       "0               -1.616644  68.271962  26.051143 2003-01-01\n",
       "1               -0.892126  68.271962  26.051143 2003-02-01\n",
       "2               -2.181170  68.271962  26.051143 2003-03-01\n",
       "3               -0.756670  68.271962  26.051143 2003-04-01\n",
       "4                0.975714  68.271962  26.051143 2003-05-01\n",
       "...                   ...        ...        ...        ...\n",
       "95503           -1.008954  79.500903  27.398616 2023-08-01\n",
       "95504           -4.776188  79.500903  27.398616 2023-09-01\n",
       "95505          -10.928502  79.500903  27.398616 2023-10-01\n",
       "95506          -18.515027  79.500903  27.398616 2023-11-01\n",
       "95507          -23.875206  79.500903  27.398616 2023-12-01\n",
       "\n",
       "[95508 rows x 4 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f0ff3b40-c149-486f-865f-265969190f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'Value' column: 252\n"
     ]
    }
   ],
   "source": [
    "nan_count = final_df[\"lwe_thickness_mean\"].isna().sum()\n",
    "print(f\"Number of NaN values in 'Value' column: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "95aff966-07ab-4340-bbc3-88dcf5953081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       lwe_thickness_mean  Longitude   Latitude       Date\n",
      "0               -1.616644  68.271962  26.051143 2003-01-01\n",
      "1               -0.892126  68.271962  26.051143 2003-02-01\n",
      "2               -2.181170  68.271962  26.051143 2003-03-01\n",
      "3               -0.756670  68.271962  26.051143 2003-04-01\n",
      "4                0.975714  68.271962  26.051143 2003-05-01\n",
      "...                   ...        ...        ...        ...\n",
      "95251           -1.008954  79.500903  27.398616 2023-08-01\n",
      "95252           -4.776188  79.500903  27.398616 2023-09-01\n",
      "95253          -10.928502  79.500903  27.398616 2023-10-01\n",
      "95254          -18.515027  79.500903  27.398616 2023-11-01\n",
      "95255          -23.875206  79.500903  27.398616 2023-12-01\n",
      "\n",
      "[95256 rows x 4 columns]\n",
      "Number of NaN values in 'Value' column: 0\n"
     ]
    }
   ],
   "source": [
    "# Define a small tolerance to account for floating point precision issues\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Remove rows where Longitude and Latitude match within the tolerance range\n",
    "final_df = final_df[\n",
    "    ~(\n",
    "        (final_df[\"Longitude\"].sub(75.458484).abs() < tolerance) & \n",
    "        (final_df[\"Latitude\"].sub(22.008724).abs() < tolerance)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Reset index after removal\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Display updated dataframe\n",
    "print(final_df)\n",
    "\n",
    "\n",
    "nan_count = final_df[\"lwe_thickness_mean\"].isna().sum()\n",
    "print(f\"Number of NaN values in 'Value' column: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "00e20fee-813d-4970-bdfe-0c5ed9c9fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [lwe_thickness_mean, Longitude, Latitude, Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'Value' column is NaN\n",
    "nan_rows = final_df[final_df['lwe_thickness_mean'].isna()]\n",
    "\n",
    "# Display the rows with NaN values in the 'Value' column\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "83983e1a-9fd3-41a8-99f2-26a75ae632e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('cleaned_grace.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f6dce-99e9-4f91-9e62-ea9067f80bf0",
   "metadata": {},
   "source": [
    "# TERRA CLIMATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "352d12e1-8793-4360-a6cd-5a1477053c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful! DataFrame shape: (156492, 8)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import re  # To extract year and month from filenames\n",
    "\n",
    "# Define TIFF folder path\n",
    "tiff_folder = \"TIFF_DATASET/terra_climate_tiff\"\n",
    "\n",
    "# Get all TIFF files\n",
    "tiff_files = sorted(glob.glob(os.path.join(tiff_folder, \"*.tif\")))\n",
    "\n",
    "# Debug: Ensure files are found\n",
    "if not tiff_files:\n",
    "    raise FileNotFoundError(\"No TIFF files found. Check folder path and file format.\")\n",
    "\n",
    "# Initialize data list\n",
    "data_list = []\n",
    "\n",
    "for tiff_file in tiff_files:\n",
    "    # Extract year and month from filename using regex\n",
    "    match = re.search(r\"_(\\d{4})_(\\d{1,2})\", os.path.basename(tiff_file))\n",
    "    if match:\n",
    "        year, month = match.groups()\n",
    "        date_str = f\"{year}-{int(month):02d}\"  # Format as YYYY-MM\n",
    "    else:\n",
    "        print(f\"Skipping file (invalid name format): {tiff_file}\")\n",
    "        continue\n",
    "\n",
    "    # Open TIFF file\n",
    "    dataset = gdal.Open(tiff_file)\n",
    "    if dataset is None:\n",
    "        print(f\"Error: Could not open {tiff_file}\")\n",
    "        continue\n",
    "\n",
    "    # Extract all bands\n",
    "    bands = [dataset.GetRasterBand(i+1) for i in range(dataset.RasterCount)]\n",
    "    \n",
    "    # Read each band data\n",
    "    band_arrays = [band.ReadAsArray() for band in bands]\n",
    "    \n",
    "    # If any band has no data, skip this file\n",
    "    if any(array is None for array in band_arrays):\n",
    "        print(f\"Error: One or more bands have no data in {tiff_file}\")\n",
    "        continue\n",
    "\n",
    "    # Get band names from the TIFF metadata (if available)\n",
    "    band_names = [band.GetDescription() for band in bands]\n",
    "\n",
    "    # If no band names are available, assign default names (e.g., Band_1, Band_2, etc.)\n",
    "    if not all(band_names):\n",
    "        band_names = [f\"Band_{i+1}\" for i in range(len(bands))]\n",
    "\n",
    "    transform = dataset.GetGeoTransform()\n",
    "    \n",
    "    rows, cols = band_arrays[0].shape\n",
    "    latitudes = np.array([transform[3] + row * transform[5] for row in range(rows)])\n",
    "    longitudes = np.array([transform[0] + col * transform[1] for col in range(cols)])\n",
    "    \n",
    "    lon_mesh, lat_mesh = np.meshgrid(longitudes, latitudes)\n",
    "    \n",
    "    # Create a DataFrame with band names as column headers\n",
    "    df = pd.DataFrame({\n",
    "        \"Longitude\": lon_mesh.ravel(),\n",
    "        \"Latitude\": lat_mesh.ravel(),\n",
    "        \"Date\": date_str\n",
    "    })\n",
    "    \n",
    "    # Add each band as a column with its respective name\n",
    "    for i, array in enumerate(band_arrays):\n",
    "        df[band_names[i]] = array.ravel()\n",
    "    \n",
    "    data_list.append(df)\n",
    "\n",
    "# Check if data_list has data before concatenating\n",
    "if not data_list:\n",
    "    raise ValueError(\"No valid data was extracted from TIFF files.\")\n",
    "\n",
    "final_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Optional: Save the dataframe to a CSV file\n",
    "# final_df.to_csv(\"terra_climate_tiff_data.csv\", index=False)\n",
    "\n",
    "print(\"Conversion successful! DataFrame shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "55e21307-2f7a-45e4-b41a-6f87503d9b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Date</th>\n",
       "      <th>aet_sum</th>\n",
       "      <th>ro_sum</th>\n",
       "      <th>tmmn_mean</th>\n",
       "      <th>tmmx_mean</th>\n",
       "      <th>soil_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.822804</td>\n",
       "      <td>31.890193</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68.271962</td>\n",
       "      <td>31.890193</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.721119</td>\n",
       "      <td>31.890193</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.170277</td>\n",
       "      <td>31.890193</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.619435</td>\n",
       "      <td>31.890193</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156487</th>\n",
       "      <td>77.704272</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156488</th>\n",
       "      <td>78.153430</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156489</th>\n",
       "      <td>78.602587</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156490</th>\n",
       "      <td>79.051745</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156491</th>\n",
       "      <td>79.500903</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156492 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Longitude   Latitude     Date  aet_sum  ro_sum  tmmn_mean  tmmx_mean  \\\n",
       "0       67.822804  31.890193  2003-01      NaN     NaN        NaN        NaN   \n",
       "1       68.271962  31.890193  2003-01      NaN     NaN        NaN        NaN   \n",
       "2       68.721119  31.890193  2003-01      NaN     NaN        NaN        NaN   \n",
       "3       69.170277  31.890193  2003-01      NaN     NaN        NaN        NaN   \n",
       "4       69.619435  31.890193  2003-01      NaN     NaN        NaN        NaN   \n",
       "...           ...        ...      ...      ...     ...        ...        ...   \n",
       "156487  77.704272  22.008724  2023-12      NaN     NaN        NaN        NaN   \n",
       "156488  78.153430  22.008724  2023-12      NaN     NaN        NaN        NaN   \n",
       "156489  78.602587  22.008724  2023-12      NaN     NaN        NaN        NaN   \n",
       "156490  79.051745  22.008724  2023-12      NaN     NaN        NaN        NaN   \n",
       "156491  79.500903  22.008724  2023-12      NaN     NaN        NaN        NaN   \n",
       "\n",
       "        soil_mean  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "...           ...  \n",
       "156487        NaN  \n",
       "156488        NaN  \n",
       "156489        NaN  \n",
       "156490        NaN  \n",
       "156491        NaN  \n",
       "\n",
       "[156492 rows x 8 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a348c1c4-41ff-4d44-974e-33dc8c8430c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JT00007\\AppData\\Local\\Temp\\ipykernel_12160\\853554686.py:18: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  final_gdf = final_gdf[final_gdf.within(shapefile.unary_union)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Longitude   Latitude     Date  aet_sum  ro_sum  tmmn_mean  tmmx_mean  \\\n",
      "40      73.661853  31.441035  2003-01     78.0     0.0       34.0      193.0   \n",
      "41      74.111011  31.441035  2003-01     95.0     1.0       37.0      193.0   \n",
      "42      74.560169  31.441035  2003-01    128.0     1.0       39.0      195.0   \n",
      "65      72.763538  30.991877  2003-01     70.0     0.0       35.0      199.0   \n",
      "66      73.212696  30.991877  2003-01     76.0     0.0       35.0      197.0   \n",
      "...           ...        ...      ...      ...     ...        ...        ...   \n",
      "156478  73.661853  22.008724  2023-12    269.0     0.0      141.0      320.0   \n",
      "156479  74.111011  22.008724  2023-12    119.0     0.0      129.0      305.0   \n",
      "156480  74.560169  22.008724  2023-12    105.0     0.0      133.0      312.0   \n",
      "156481  75.009326  22.008724  2023-12    110.0     0.0      135.0      316.0   \n",
      "156482  75.458484  22.008724  2023-12      NaN     NaN        NaN        NaN   \n",
      "\n",
      "        soil_mean  \n",
      "40           10.0  \n",
      "41           43.0  \n",
      "42           96.0  \n",
      "65            5.0  \n",
      "66            4.0  \n",
      "...           ...  \n",
      "156478      557.0  \n",
      "156479      326.0  \n",
      "156480      279.0  \n",
      "156481      261.0  \n",
      "156482        NaN  \n",
      "\n",
      "[95508 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the shapefile (update the file path)\n",
    "shapefile_path = \"RJ_150KM_Buff_Bnd/IND_Rajasthan_Buffer_150km.shp\"\n",
    "shapefile = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Convert final_df to a GeoDataFrame\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(final_df['Longitude'], final_df['Latitude'])]\n",
    "final_gdf = gpd.GeoDataFrame(final_df, geometry=geometry, crs=\"EPSG:4326\")  # Assuming WGS84 projection\n",
    "\n",
    "# Ensure the shapefile is in the same CRS\n",
    "if shapefile.crs != final_gdf.crs:\n",
    "    shapefile = shapefile.to_crs(final_gdf.crs)\n",
    "\n",
    "# Perform spatial join to filter points inside the shapefile boundary\n",
    "final_gdf = final_gdf[final_gdf.within(shapefile.unary_union)]\n",
    "\n",
    "# Convert back to DataFrame (if you don't need the geometry column)\n",
    "final_df = final_gdf.drop(columns=['geometry'])\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8a5d218e-c1f6-445a-89a1-3feee8006cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in each column:\n",
      "aet_sum      252\n",
      "ro_sum       252\n",
      "tmmn_mean    252\n",
      "tmmx_mean    252\n",
      "soil_mean    252\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Counting the NaN values in each of the band columns (aet_sum, ro_sum, tmmn_mean, tmmx_mean, soil_mean)\n",
    "nan_count = final_df[['aet_sum', 'ro_sum', 'tmmn_mean', 'tmmx_mean', 'soil_mean']].isna().sum()\n",
    "\n",
    "# Print NaN count for each band\n",
    "print(\"Number of NaN values in each column:\")\n",
    "print(nan_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "686c21fe-1992-4981-9fbe-be007c989266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Longitude   Latitude     Date  aet_sum  ro_sum  tmmn_mean  tmmx_mean  \\\n",
      "611     75.458484  22.008724  2003-01      NaN     NaN        NaN        NaN   \n",
      "1232    75.458484  22.008724  2003-02      NaN     NaN        NaN        NaN   \n",
      "1853    75.458484  22.008724  2003-03      NaN     NaN        NaN        NaN   \n",
      "2474    75.458484  22.008724  2003-04      NaN     NaN        NaN        NaN   \n",
      "3095    75.458484  22.008724  2003-05      NaN     NaN        NaN        NaN   \n",
      "...           ...        ...      ...      ...     ...        ...        ...   \n",
      "153998  75.458484  22.008724  2023-08      NaN     NaN        NaN        NaN   \n",
      "154619  75.458484  22.008724  2023-09      NaN     NaN        NaN        NaN   \n",
      "155240  75.458484  22.008724  2023-10      NaN     NaN        NaN        NaN   \n",
      "155861  75.458484  22.008724  2023-11      NaN     NaN        NaN        NaN   \n",
      "156482  75.458484  22.008724  2023-12      NaN     NaN        NaN        NaN   \n",
      "\n",
      "        soil_mean  \n",
      "611           NaN  \n",
      "1232          NaN  \n",
      "1853          NaN  \n",
      "2474          NaN  \n",
      "3095          NaN  \n",
      "...           ...  \n",
      "153998        NaN  \n",
      "154619        NaN  \n",
      "155240        NaN  \n",
      "155861        NaN  \n",
      "156482        NaN  \n",
      "\n",
      "[252 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where any of the specified columns have NaN values\n",
    "nan_rows = final_df[final_df[['aet_sum', 'ro_sum', 'tmmn_mean', 'tmmx_mean', 'soil_mean']].isna().any(axis=1)]\n",
    "\n",
    "# Display the rows with NaN values in any of the specified columns\n",
    "print(nan_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "307ef461-e8e0-4987-a644-7d18fd950621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Longitude   Latitude     Date  aet_sum  ro_sum  tmmn_mean  tmmx_mean  \\\n",
      "0      73.661853  31.441035  2003-01     78.0     0.0       34.0      193.0   \n",
      "1      74.111011  31.441035  2003-01     95.0     1.0       37.0      193.0   \n",
      "2      74.560169  31.441035  2003-01    128.0     1.0       39.0      195.0   \n",
      "3      72.763538  30.991877  2003-01     70.0     0.0       35.0      199.0   \n",
      "4      73.212696  30.991877  2003-01     76.0     0.0       35.0      197.0   \n",
      "...          ...        ...      ...      ...     ...        ...        ...   \n",
      "95251  75.907642  22.457882  2023-12    267.0     0.0      131.0      313.0   \n",
      "95252  73.661853  22.008724  2023-12    269.0     0.0      141.0      320.0   \n",
      "95253  74.111011  22.008724  2023-12    119.0     0.0      129.0      305.0   \n",
      "95254  74.560169  22.008724  2023-12    105.0     0.0      133.0      312.0   \n",
      "95255  75.009326  22.008724  2023-12    110.0     0.0      135.0      316.0   \n",
      "\n",
      "       soil_mean  \n",
      "0           10.0  \n",
      "1           43.0  \n",
      "2           96.0  \n",
      "3            5.0  \n",
      "4            4.0  \n",
      "...          ...  \n",
      "95251      494.0  \n",
      "95252      557.0  \n",
      "95253      326.0  \n",
      "95254      279.0  \n",
      "95255      261.0  \n",
      "\n",
      "[95256 rows x 8 columns]\n",
      "Number of NaN values in each column:\n",
      "aet_sum      0\n",
      "ro_sum       0\n",
      "tmmn_mean    0\n",
      "tmmx_mean    0\n",
      "soil_mean    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define a small tolerance to account for floating point precision issues\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Remove rows where Longitude and Latitude match within the tolerance range\n",
    "final_df = final_df[\n",
    "    ~(\n",
    "        (final_df[\"Longitude\"].sub(75.458484).abs() < tolerance) & \n",
    "        (final_df[\"Latitude\"].sub(22.008724).abs() < tolerance)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Reset index after removal\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "# Display updated dataframe\n",
    "print(final_df)\n",
    "\n",
    "# Counting the NaN values in each of the specified columns (aet_sum, ro_sum, tmmn_mean, tmmx_mean, soil_mean)\n",
    "nan_count = final_df[['aet_sum', 'ro_sum', 'tmmn_mean', 'tmmx_mean', 'soil_mean']].isna().sum()\n",
    "\n",
    "# Print NaN count for each column\n",
    "print(\"Number of NaN values in each column:\")\n",
    "print(nan_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f8aba1ba-3d18-45f9-948e-5659021d264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in each column:\n",
      "aet_sum      0\n",
      "ro_sum       0\n",
      "tmmn_mean    0\n",
      "tmmx_mean    0\n",
      "soil_mean    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Counting the NaN values in each of the band columns (aet_sum, ro_sum, tmmn_mean, tmmx_mean, soil_mean)\n",
    "nan_count = final_df[['aet_sum', 'ro_sum', 'tmmn_mean', 'tmmx_mean', 'soil_mean']].isna().sum()\n",
    "\n",
    "# Print NaN count for each band\n",
    "print(\"Number of NaN values in each column:\")\n",
    "print(nan_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "49acb469-7ea4-454c-8cd4-0558a89ada66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Date</th>\n",
       "      <th>aet_sum</th>\n",
       "      <th>ro_sum</th>\n",
       "      <th>tmmn_mean</th>\n",
       "      <th>tmmx_mean</th>\n",
       "      <th>soil_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.661853</td>\n",
       "      <td>31.441035</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74.111011</td>\n",
       "      <td>31.441035</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74.560169</td>\n",
       "      <td>31.441035</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72.763538</td>\n",
       "      <td>30.991877</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.212696</td>\n",
       "      <td>30.991877</td>\n",
       "      <td>2003-01</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95251</th>\n",
       "      <td>75.907642</td>\n",
       "      <td>22.457882</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95252</th>\n",
       "      <td>73.661853</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>557.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95253</th>\n",
       "      <td>74.111011</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95254</th>\n",
       "      <td>74.560169</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95255</th>\n",
       "      <td>75.009326</td>\n",
       "      <td>22.008724</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95256 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Longitude   Latitude     Date  aet_sum  ro_sum  tmmn_mean  tmmx_mean  \\\n",
       "0      73.661853  31.441035  2003-01     78.0     0.0       34.0      193.0   \n",
       "1      74.111011  31.441035  2003-01     95.0     1.0       37.0      193.0   \n",
       "2      74.560169  31.441035  2003-01    128.0     1.0       39.0      195.0   \n",
       "3      72.763538  30.991877  2003-01     70.0     0.0       35.0      199.0   \n",
       "4      73.212696  30.991877  2003-01     76.0     0.0       35.0      197.0   \n",
       "...          ...        ...      ...      ...     ...        ...        ...   \n",
       "95251  75.907642  22.457882  2023-12    267.0     0.0      131.0      313.0   \n",
       "95252  73.661853  22.008724  2023-12    269.0     0.0      141.0      320.0   \n",
       "95253  74.111011  22.008724  2023-12    119.0     0.0      129.0      305.0   \n",
       "95254  74.560169  22.008724  2023-12    105.0     0.0      133.0      312.0   \n",
       "95255  75.009326  22.008724  2023-12    110.0     0.0      135.0      316.0   \n",
       "\n",
       "       soil_mean  \n",
       "0           10.0  \n",
       "1           43.0  \n",
       "2           96.0  \n",
       "3            5.0  \n",
       "4            4.0  \n",
       "...          ...  \n",
       "95251      494.0  \n",
       "95252      557.0  \n",
       "95253      326.0  \n",
       "95254      279.0  \n",
       "95255      261.0  \n",
       "\n",
       "[95256 rows x 8 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6fe2a063-8f60-4328-95a1-be614adf65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('cleaned_terra_climate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "29f4e94f-f770-4ce2-a2b2-138f6c57f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pip install geopandas shapely --trusted-host pypi.org --trusted-host files.pythonhosted.org --proxy http://rcwest-student3:NRSC@User@192.168.0.10:8080"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
